## 공식 문제 세트 20문항

**1. 한 데이터 엔지니어가 AWS Glue 추출, 변환 및 로드(ETL) 파이프라인을 배포하고자 새 계정을 생성했다. 파이프라인 작업을 통해 소스 Amazon S3 버킷에서 원시 데이터를 수집해야 한다. 그런 다음 파이프라인 작업에서, 변환된 데이터를 동일한 계정의 대상 S3 버킷에 작성한다. 데이터 엔지니어는 AWS Glue가 소스 S3 버킷과 대상 S3 버킷에 액세스할 수 있는 권한이 포함된 IAM 정책을 작성했다. ETL 파이프라인을 실행하려면 데이터 엔지니어가 IAM 정책의 권한을 AWS Glue에 부여해야 한다. 다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇인가?** 

- AWS Glue에 새 IAM 서비스 역할을 만든다. 정책을 새 역할에 연결한다. 새 역할을 사용하도록 AWS Glue를 구성한다.

**2. 전자 상거래 회사가 AWS에서 여러 애플리케이션을 실행한다. 이 회사는 중앙 집중식 스트리밍 로그 수집 솔루션을 설계하려고 한다. 솔루션을 사용해 로그 파일을 Apache Parquet 형식으로 변환할 수 있어야 한다. 그런 다음 솔루션으로 Amazon S3에 로그 파일을 저장할 수 있어야 한다. 생성되는 로그 파일의 수는 하루 중에 계속 달라진다. 데이터 엔지니어는 로그 파일을 거의 실시간으로 전달하도록 지원하는 솔루션을 구성해야 한다. 다음 중 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- 로그 파일을 Amazon Kinesis Data Firehose에 전송하도록 애플리케이션을 구성한다. 로그 파일을 Parquet 형식으로 변환하는 AWS Lambda 함수를 호출하도록 Kinesis Data Firehose를 구성한다. Parquet 파일을 출력 S3 버킷에 전송하도록 Kinesis Data Firehose를 구성한다.
  ![image](https://github.com/seonwook97/Certificate/assets/92377162/07453d53-353e-458e-ba7f-377e299116bf)

**3. 한 전자 상거래 회사가 AWS에서 애플리케이션을 실행하고 있다. 이 애플리케이션은 Amazon Redshift의 테이블에서 최신 데이터를 얻고 있다. 1년 이상 된 데이터는 Amazon S3에서 액세스할 수 있다. 최근 SQL로 작성된 새로운 보고서가 있다. 보고서를 비교하려면 올해 매출 테이블의 일부 열과 전년도 매출 데이터가 포함된 테이블에서 동일한 열을 살펴봐야 한다. 보고서 실행 속도가 느리고 성능이 떨어지며 결과를 얻기까지 기다리는 시간이 길다. 데이터 엔지니어가 쿼리를 가속화하려면 백엔드 스토리지를 최적화해야 한다. 다음 중 가장 효과적으로 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- Amazon S3에서 데이터를 수집하도록 보고서 SQL 스테이트먼트를 실행한다. Amazon Redshift 구체화된 뷰에 결과 세트를 저장한다. SQL REFRESH를 실행하도록 보고서를 구성한다. 그런 다음 구체화된 뷰를 쿼리한다.
  - Redshift 물리화된 뷰(Materialized Views)
    - 하나 이상의 기본 테이블에 대한 SQL 쿼리를 기반으로 사전 계산된 결과를 포함합니다.
    - 이는 "일반" 뷰와 달리 실제로 쿼리 결과를 저장한다는 점에서 다릅니다.
    - 특히 대규모 테이블에서 데이터 웨어하우스 환경의 복잡한 쿼리를 가속화하는 방법을 제공합니다.
    - 물리화된 뷰는 다른 테이블이나 뷰처럼 쿼리할 수 있습니다.
    - 쿼리는 기본 테이블에 접근하지 않고 사전 계산된 결과를 사용하므로 더 빠르게 결과를 반환합니다.
    - 이는 예측 가능하고 반복적인 쿼리, 예를 들어 Amazon QuickSight와 같은 대시보드 채우기에 특히 유용합니다.
    ![image](https://github.com/seonwook97/Certificate/assets/92377162/e7b0c7cc-28c3-4b98-bf9e-1ff2a0ae3feb)


**4. 회사에서 데이터를 저장하는 데 Amazon S3 버킷을 활용한다. 이 회사는 새로운 데이터 수명 주기 및 보존 정책을 채택하는 중이다. 정책 정의는 다음과 같다.**
**- 새로 생성된 데이터는 모두 온라인에서 사용할 수 있어야 하고 경우에 따라 SQL로 분석을 수행할 수 있어야 한다.**
**- 3년이 지난 데이터는 안전하게 보관하고 규정 준수 평가를 위해 필요할 때 12시간 이내에 사용할 수 있어야 한다.**
**- 10년 이상 된 데이터는 안전하게 삭제해야 한다.**
**데이터 엔지니어는 데이터 수명 주기 및 보존 정책에 따라 데이터를 비용 효율적으로 저장할 수 있는 솔루션을 구성해야 한다. 다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- S3 Infrequent Access 스토리지 클래스에 새 데이터를 저장한다. Amazon Athena를 사용하여 Amazon S3에 있는 데이터를 해당 위치에서 쿼리한다. 데이터를 3년 후에 S3 Glacier Flexible Retrieval 스토리지 클래스로 마이그레이션하는 수명 주기 규칙을 만든다. 10년 후에 데이터를 삭제하는 수명 주기 규칙을 구성한다.

**5. 회사가 AWS Glue를 사용하여 레코드를 처리하는 데이터 파이프라인을 배포했다. 레코드에는 JSON 형식의 이벤트가 포함되며 경우에 따라 base64로 인코딩된 이미지가 포함되기도 한다. AWS Glue 작업은 10개의 데이터 처리 단위(DPU)로 구성된다. 하지만 AWS Glue 작업은 정기적으로 수백 개의 DPU로 확장되며 실행하는 데 시간이 오래 걸릴 수 있다. 데이터 엔지니어는 데이터 파이프라인을 모니터링하여 적절한 DPU 용량을 결정해야 한다. 다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- AWS Glue 콘솔의 작업 실행 모니터링 섹션을 검사한다. 이전 작업 실행 결과를 검토한다. 적절한 DPU 수를 결정하도록 프로파일링된 지표를 시각화한다. 
  - Glue ETL
    - 기본 Spark 작업의 성능을 향상시키기 위해 추가 "DPU"(데이터 처리 단위)를 프로비저닝할 수 있음
    - 작업 메트릭을 활성화하면 필요한 최대 DPU 용량을 이해하는 데 도움이 됨

**6. 회사가 여러 운영 소스에서 Amazon S3 데이터 레이크로 데이터를 수집한다. 그런 다음 회사는 비즈니스 분석팀이 분석할 수 있도록 데이터를 Amazon Redshift에 수집한다. 비즈니스 분석팀은 최근 3개월간의 고객 데이터에만 액세스할 수 있어야 한다. 또한 회사는 일 년에 한 번 전년도 데이터를 자세히 분석하여 나온 결과를 지난 12개월의 전체 결과와 비교한다. 분석 및 비교한 후에는 더 이상 데이터에 액세스할 수 없다. 그러나 규정을 준수하려면 데이터를 12개월 이후에도 보관해야 한다. 다음 중 가장 비용 효율적인 방식으로 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- 3개월간의 데이터를 Amazon Redshift에 수집한다. 3개월이 지난 데이터는 Amazon Redshift에서 Amazon S3로 언로드 프로세스를 자동화한다. 최대 12개월 이전의 데이터를 포함하여 연간 분석을 수행하는 데 Amazon Redshift Spectrum을 사용한다. Amazon S3에서 수명 주기 정책을 구현하여 12개월이 지난 데이터의 경우, 언로드된 데이터를 S3 Glacier Deep Archive로 이전한다.
  - Redshift Spectrum
    - 데이터를 로드하지 않고 S3에 있는 엑사바이트 규모의 비정형 데이터를 쿼리
    - 무제한 동시성
    - 수평 확장
    - 스토리지와 컴퓨팅 리소스의 분리
    - 다양한 데이터 형식 지원
    - Gzip 및 Snappy 압축 지원
    ![image](https://github.com/seonwook97/Certificate/assets/92377162/9db5f584-e09a-4282-a771-aef4944219ef)

**7. 회사의 데이터가 온프레미스 NFS 파일 공유에 있다. 회사는 AWS로 마이그레이션할 계획이며, 데이터 분석에 데이터가 사용된다. 이 회사는 AWS Lambda 함수를 작성해 데이터를 분석한다. 그리고 Lambda가 액세스하는 파일 시스템에 NFS를 계속 사용하고자 한다. 동시에 실행되는 모든 Lambda 함수 전반에서 데이터가 공유되어야 한다. 다음 중 회사가 이 데이터 마이그레이션에 사용해야 할 솔루션은 무엇인가?**

- 데이터를 Amazon Elastic File System(Amazon EFS)으로 마이그레이션한다. 파일 시스템을 마운트할 수 있도록 Lambda 함수를 구성한다.
  - Amazon EFS – 탄력적 파일 시스템
    - 여러 EC2 인스턴스에 마운트할 수 있는 관리형 NFS(네트워크 파일 시스템)
    - EFS는 여러 가용 영역(AZ)에 걸쳐 EC2 인스턴스와 함께 작동
    - 고가용성, 확장 가능, 비용이 많이 듦(gp2의 3배), 사용량 기준 과금
      ![image](https://github.com/seonwook97/Certificate/assets/92377162/5f86b919-f1bd-47cb-acad-828c9ca3b0a2)

**8. 회사는 사용자가 만든 데이터를 분석용으로 수집하는 데 Amazon S3 데이터 레이크를 사용한다. Amazon S3에서 수집 및 저장되는 일부 데이터에는 개인 식별 정보(PII) 가 포함된다. 이 회사는 데이터 엔지니어가 분석을 수행하기 전에 자동화 솔루션을 설계하여 신규 및 기존 데이터에 포함된 PII에서 마스킹되어야 할 데이터를 식별하기를 바란다. 또한 데이터 엔지니어는 식별된 데이터의 개요를 제공해야 한다. 데이터를 마스킹하는 작업은 AWS 계정에 이미 생성된 애플리케이션에서 처리된다. 데이터 엔지니어는 PII를 찾을 때 실시간으로 이 애플리케이션을 호출할 수 있는 솔루션을 설계해야 한다. 다음 중 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- AWS 계정에서 Amazon Macie를 활성화한다. Macie 결과에 따라 기본 이벤트 버스의 Amazon EventBridge 규칙을 만든다. 마스킹 애플리케이션을 규칙의 대상으로 설정한다.
  - AWS Macie
    - Amazon Macie는 기계 학습과 패턴 매칭을 사용하여 AWS의 민감한 데이터를 발견하고 보호하는 완전 관리형 데이터 보안 및 데이터 프라이버시 서비스입니다.
    - Macie는 개인 식별 정보(PII)와 같은 민감한 데이터를 식별하고 경고하는 데 도움을 줍니다.
      ![image](https://github.com/seonwook97/Certificate/assets/92377162/10297b05-5569-48bf-973c-4febf5821f2a)

**9. AWS에 중앙 집중식 메타데이터 스토리지 솔루션을 배포해야 하는 데이터 엔지니어가 있다. 솔루션은 안정적이고 확장 가능해야 하며, 데이터베이스, 테이블, 열, 행 및 셀 수준에서 세분화된 권한을 제어할 수 있어야 한다. 다음 중 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- 데이터 레이크와 데이터 카탈로그를 생성할 수 있도록 AWS Lake Formation을 사용한다. Lake Formation 데이터 필터를 사용하여 액세스를 제어한다.

  - AWS Lake Formation
    - “며칠 내에 안전한 데이터 레이크를 설정할 수 있도록 도와줍니다”
    - 데이터 로드 및 데이터 흐름 모니터링
    - 파티션 설정
    - 암호화 및 키 관리
    - 변환 작업 정의 및 모니터링
    - 액세스 제어
    - 감사
    - Glue 기반 구축
    ![image](https://github.com/seonwook97/Certificate/assets/92377162/7e7f849a-e967-46fe-af04-5f08418da03c)

**10. 한 금융 회사가 투자 전략 강화를 위해 기계 학습(ML) 모델을 개발했다. 이 모델에서는 주식, 채권 및 원자재 시장과 관련된 다양한 데이터 소스가 사용된다. 모델은 프로덕션 승인을 받았다. 데이터 엔지니어는 ML 의사결정을 실행하는 데 사용되는 데이터가 정확하고 완전하며 신뢰할 수 있는지를 확인해야 한다. 데이터 엔지니어는 모델 프로덕션 배포를 대비한 데이터 준비를 자동화해야 한다. 다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- 모델에 사용할 데이터를 준비할 수 있도록 Amazon SageMaker ML 계보 추적 단계와 함께 Amazon SageMaker 워크플로를 사용한다.
  - SageMaker ML 계보 추적
    - ML 워크플로우(MLOps)를 생성하고 저장합니다
    - 모델의 실행 기록을 유지합니다
    - 감사 및 준수를 위한 추적
    - 자동 또는 수동으로 생성된 추적 엔터티
    - 계정 간 계보를 위한 AWS Resource Access Manager와 통합
    - SageMaker에서 생성된 계보 그래프 샘플
    ![image](https://github.com/seonwook97/Certificate/assets/92377162/cec5f721-072a-4f71-a10f-d4b5a993b3b0)

**11. 회사가 AWS에서 Amazon Redshift 데이터 웨어하우스를 운영하고 있다. 이 회사는 최근 여러 AWS 서비스에서 지원하는 서비스형 소프트웨어(SaaS) 영업 애플리케이션의 사용을 시작했다. 그리고 보고 목적으로 SaaS 애플리케이션의 일부 데이터를 Amazon Redshift로 전송하고자 한다. 데이터 엔지니어는 SaaS 애플리케이션에서 Amazon Redshift로 데이터를 지속적으로 전송할 수 있는 솔루션을 구성해야 한다. 다음 중 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- 선택된 소스 데이터를 Amazon Redshift로 수집할 수 있도록 Amazon AppFlow 플로를 만든다. 이벤트에서 실행되도록 플로를 구성한다.
  - Amazon AppFlow
    - SaaS(Software-as-a-Service) 애플리케이션과 AWS 간에 데이터를 안전하게 전송할 수 있는 완전 관리형 통합 서비스입니다
    - 소스: Salesforce, SAP, Zendesk, Slack, ServiceNow
    - 대상: Amazon S3, Amazon Redshift와 같은 AWS 서비스 또는 SnowFlake, Salesforce와 같은 비AWS 서비스
    - 빈도: 일정에 따라, 이벤트에 응답하여, 또는 필요할 때마다
    - 필터링 및 검증과 같은 데이터 변환 기능
    - 공용 인터넷을 통한 암호화 또는 AWS PrivateLink를 통한 개인 네트워크를 통한 전송
    - 통합 작업을 직접 작성하는 데 시간을 낭비하지 않고 API를 즉시 활용할 수 있습니다
    ![image](https://github.com/seonwook97/Certificate/assets/92377162/ea5acca3-884d-4286-893c-5bcb2a554790)

**12. 한 금융 회사가 Amazon S3 버킷에 지불이 끝난 인보이스를 저장하고 있다. 인보이스가 업로드되면 AWS Lambda 함수에서 Amazon Textract를 사용하여 PDF 데이터를 처리하고 데이터를 Amazon DynamoDB에 보관한다. 현재 Lambda 실행 역할에는 다음의 S3 권한이 있다.**
```JSON
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "ExampleStmt",
            "Action": ["s3:*”],
            "Effect": "Allow",
            "Resource": ["*"]
        }

    ]

}
```
**이 회사는 보안 모범 사례에 따라 Amazon S3에 역할 권한을 지정하여 수정하고자 한다. 다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- Action을 "s3:GetObject"로 수정한다. Resource가 버킷 ARN만 되도록 수정
  - `s3:PutObject`: 특정 S3 버킷에 객체를 업로드하는 데 사용
  - `s3:GetObject`: 특정 S3 버킷에서 객체를 다운로드하는 데 사용
  - `s3:DeleteObject`: 특정 S3 버킷에서 객체를 삭제하는 데 사용
  - `s3:ListBucket`: 특정 S3 버킷의 내용을 나열하는 데 사용
  - `s3:CopyObject`: S3 버킷 내의 객체를 다른 위치로 복사하는 데 사용
  - `s3:AbortMultipartUpload`: 멀티파트 업로드를 중단하는 데 사용
  - `s3:ListMultipartUploadParts`: 멀티파트 업로드의 일부를 나열하는 데 사용

**13. Amazon Kinesis 애플리케이션에서 Kinesis 데이터 스트림의 데이터를 읽으려고 한다. 그런데 읽기 데이터 호출이 거부되며, 다음 오류 메시지가 표시된다: ProvisionedThroughputExceededException. 다음 중 어떤 단계를 조합하여 오류를 해결할 수 있는가? (2개 선택)**

- 읽기 데이터 호출용으로 충분한 용량이 제공되도록 스트림 내에서 샤드 수를 늘린다.
- 애플리케이션이 스트림에서 데이터를 읽어 재시도하게 만든다.
  - `ProvisionedThroughputExceededException`: Amazon Kinesis 데이터 스트림 내 데이터 읽기 처리량 제한 초과
 
**14. Amazon Redshift 클러스터를 운영 중인 회사가 있다. 데이터 엔지니어는 회사가 Amazon Redshift의 별도 테스트 환경에서 분석 자료를 얻을 수 있도록 솔루션을 설계해야 한다. 솔루션은 기본 Redshift 클러스터의 데이터를 사용한다. 두 번째 클러스터는 새로운 테스트 프로세스의 일환으로 2주마다 2시간 동안만 사용될 것으로 예상하고 있다. 다음 중 가장 비용 효율적인 방식으로 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- 기본 Redshift 클러스터에서 Redshift 테스트 클러스터로의 데이터 공유를 만든다. 테스트 환경에서 Amazon Redshift 서버리스를 사용한다.
  - `Redshift Serverless` 
    - 작업량에 대한 자동 스케일링 및 프로비저닝 
    - 비용 및 성능 최적화 
    - 사용 중일 때만 비용 지불 
    - 가변적이고 간헐적인 작업량에 걸쳐 성능을 유지하기 위해 머신러닝 사용   
    - 개발 및 테스트 환경의 쉬운 구축   
    - 쉬운 ad-hoc 비즈니스 분석   
    - 서버리스 엔드포인트, JDBC/ODBC 연결을 받거나 콘솔의 쿼리 편집기를 통해 쿼리를 수행할 수 있음
  ![image](https://github.com/seonwook97/Data_Engineering/assets/92377162/47b55036-0f07-43bb-b249-aa8b4aa71ff2)

**15. 회사가 Amazon S3 데이터 레이크를 사용 중이다. 이 회사는 Amazon Kinesis Data Streams를 사용하여 데이터 레이크로 데이터를 수집한다. 그리고 AWS Lambda를 사용하여 스트림에서 들어오는 데이터를 읽고 처리한다. 수집되는 데이터의 볼륨이 매우 가변적이어서 예측할 수 없다. 현재 대량의 데이터가 스트림에 게시되는 피크 시간대에는 IteratorAge 지표가 높게 나타난다. 데이터 엔지니어는 Lambda로 Kinesis Data Streams를 읽을 때 성능을 높이는 솔루션을 설계해야 한다. 다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇인가? (3개 선택)**

- Kinesis 데이터 스트림의 샤드 수를 늘린다.
- 최적의 성능을 찾고자 다양한 병렬화 인자 설정을 테스트한다.
- 향상된 팬아웃을 통해 Lambda 함수를 소비자로 등록한다.
  - `Kinesis Data Streams`
    - 보존 기간은 1일에서 365일까지 
    - 데이터 재처리 (재생) 가능 
    - Kinesis에 데이터가 삽입되면 삭제할 수 없습니다 (불변성) 
    - 동일한 파티션을 공유하는 데이터는 동일한 샤드로 이동 (정렬) 
    - 생산자: AWS SDK, Kinesis Producer Library (KPL), Kinesis Agent 
    - 소비자: 
      - 직접 작성: Kinesis Client Library (KCL), AWS SDK 
      - 관리형: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics
  ![image](https://github.com/seonwook97/Certificate/assets/92377162/fbf6c912-ef08-41b3-ac02-426a7d7bae9e)
  
  - AWS Lambda는 Kinesis Data Streams에서 레코드를 가져올 수 있음 
    - Lambda 소비자는 KPL에서 레코드를 분해하는 라이브러리를 가지고 있음 
    - Lambda는 다음을 위한 경량 ETL을 실행하는 데 사용할 수 있음: 
      - Amazon S3 
      - DynamoDB 
      - Redshift 
      - OpenSearch 
      - 원하는 어디든지 
    - Lambda는 실시간으로 알림을 트리거하거나 이메일을 보내는 데 사용할 수 있음 
    - Lambda에는 설정 가능한 배치 크기가 있음
  
  - Kinesis Enhanced Fan Out 
    - 2018년 8월에 새롭게 도입된 게임 체인징 기능 
    - KCL 2.0 및 AWS Lambda (2018년 11월)와 함께 작동
    - 각 소비자는 샤드 당 2 MB/s의 프로비저닝 처리량을 얻음
    - 이는 20개의 소비자가 샤드 당 총 40MB/s를 얻게 됨
    - 더 이상 2 MB/s 제한이 없음 
    - Enhanced Fan Out: Kinesis가 HTTP/2를 통해 데이터를 소비자에게 푸시함 
    - 지연 시간을 줄임 (~70 ms)
  <img width="317" alt="image" src="https://github.com/seonwook97/Certificate/assets/92377162/c619b5da-9079-43d6-97df-f8f3bd74ff37">

  - Kinesis 작업 - 샤드 추가 
    - "샤드 분할"이라고도 함
    - 스트림 용량을 늘리는 데 사용할 수 있음 (샤드 당 1 MB/s 데이터 입력) 
    - "핫 샤드"를 나누는 데 사용할 수 있음 
    - 이전 샤드는 닫히며 데이터가 만료되면 삭제됨
  ![image](https://github.com/seonwook97/Certificate/assets/92377162/caded107-5037-4ab4-84a1-12c45ade71c7)

  - `Iterator`: 데이터 스트림에서 이벤트를 읽기 위한 반복자
  - `IteratorAge`: Kinesis 데이터 스트림에서 제공되는 지표로, 데이터가 스트림에서 처리되는 데 걸리는 시간
  - `Kinesis Shard`: 스트림의 데이터 처리량을 확장하고 늘리는 데 사용
  - `팬아웃(Fan-out)`: 데이터를 여러 소비자로 복사하는 것

**16. 한 컨설턴트 회사는 클라우드 기반 시간 추적 시스템을 사용하여 직원의 근무 시간을 추적한다. 전 세계에 널리 퍼져있는 이 회사의 직원은 수천 명에 이른다. 시간 추적 시스템에서는 전날의 레코드를 CSV 형식으로 가져오는 REST API를 제공한다. 이 회사는 온프레미스에 크론을 운영하고 있는데, 이 크론은 매일 아침 같은 시간에 Python 프로그램을 실행하도록 예약되어 있다. 이 프로그램은 데이터 레이크 역할을 하는 Amazon S3 버킷에 데이터를 저장한다. 데이터 엔지니어는 동일한 Python 코드와 크론 구성을 재사용하는 AWS 서비스를 갖춘 솔루션을 제공해야 한다. 다음 중 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 단계는 무엇인가? (2개 선택)**

- AWS Lambda 함수에서 Python 코드를 실행한다.
- Amazon EventBridge 스케줄러를 사용하여 크론을 예약한다.
  - `AWS Lambda`
    - 서버리스 컴퓨팅 서비스
    - 코드 실행을 위한 컴퓨팅 리소스 제공
    - 사용한 컴퓨팅 시간만큼 비용 지불
    - 관리형 서비스
    - 코드를 실행하고 스케일링, 모니터링, 로그 기록, 보안 등을 관리
    - 코드를 실행하기 위한 환경을 프로비저닝
    - 실행이 완료되면 환경을 종료
    - 이벤트에 응답하도록 설정 가능
    - 여러 AWS 서비스와 통합
    - 여러 프로그래밍 언어 지원
    - 런타임에 따라 코드 실행
    - 코드를 실행하는 데 필요한 모든 것을 제공
  
  - `Amazon EventBridge`
    - 이벤트 기반 소프트웨어를 쉽게 구축하고 자동으로 관리
    - 이벤트 기반 아키텍처를 사용하여 애플리케이션을 통합하고 자동화
    - AWS에서 제공하는 서비스 간 이벤트 전달 및 통합
    - 이벤트 소스와 대상을 연결하고 이벤트를 전달하는 규칙을 정의
    - AWS 서비스 이벤트와 사용자 생성 이벤트를 수신하고 처리
    - 사용자 지정 이벤트를 생성하고 이벤트 대상을 호출
    - 이벤트를 다른 AWS 계정으로 전달하고 다른 AWS 계정에서 이벤트 수신

**17. 한 데이터 엔지니어가 변환할 데이터를 Amazon Simple Queue Service(Amazon SQS) 대기열에 추가하는 애플리케이션을 설계 중이다. 마이크로서비스는 대기열에서 메시지를 수신한다. 데이터 엔지니어는 메시지 지속성이 유지되기를 바란다. 다음 중 어떤 이벤트가 SQS 대기열에서 메시지를 제거할 수 있는가? (3개 선택)**

- 애플리케이션이 Amazon SQS에 DeleteMessage API를 호출한다.
- 메시지가 maxReceiveCount에 도달한다.
- 대기열이 제거된다.
  - `Amazon SQS`
    - 메시지를 관리하고 전달하는 완전 관리형 메시징 서비스
    - 분산된 컴퓨팅 환경에서 메시지 전달을 위한 표준 방법
    - 메시지 큐를 사용하여 독립적인 컴포넌트 간의 통신을 통제


**18. 회사가 Amazon RDS for Microsoft SQL Server 데이터베이스 기반 Amazon EC2 인스턴스에서 클라우드 기반 소프트웨어 애플리케이션을 실행 중이다. 애플리케이션은 데이터베이스의 자격 증명 정보 및 레코드를 수집, 처리 및 저장한다. 회사는 자격 증명 노출 위험을 제거하고자 한다. 다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- AWS Secrets Manager를 사용하여 자격 증명을 저장한다. 30일마다 자격 증명 정보를 교체하도록 Secrets Manager에서 자동 교체 기능을 구성한다.
  - `AWS Secrets Manager`
    - API를 사용하여 암호, 토큰, 데이터베이스 연결 문자열 및 기타 중요 정보를 보호
    - 애플리케이션, 서비스 및 IT 리소스에 대한 비밀 정보를 중앙에서 관리
    - 비밀 정보를 암호화하고 복호화
    - 비밀 정보를 자동으로 로테이션
    - 비밀 정보에 대한 액세스 제어 및 권한 부여
    - 비밀 정보에 대한 감사 및 모니터링
    - 비밀 정보에 대한 알림 및 로깅

**19. 데이터 엔지니어가 Amazon Elastic Kubernetes Service(Amazon EKS)에서 관리하는 컨테이너의 데이터를 변환하는 애플리케이션을 설계하는 중이다. 컨테이너는 Amazon EC2 노드에서 실행된다. 컨테이너화된 각 애플리케이션은 독립적인 데이터 집합을 변환한 다음 데이터 레이크에 데이터를 저장한다. 데이터를 다른 컨테이너와 공유할 필요는 없다. 데이터 엔지니어는 변환이 완료되기 전에 데이터를 저장할 위치를 정해야 한다. 다음 중 최소한의 지연 시간으로 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- 컨테이너는 노드의 RAM에서 제공하는 임시 볼륨을 사용해야 한다.
  - `Amazon EKS`
    - AWS에서 Kubernetes를 실행하는 완전 관리형 서비스
    - Kubernetes의 컨테이너 관리 기능과 AWS의 확장성, 보안 및 안정성을 결합
    - 워크로드를 실행하는 데 필요한 리소스를 프로비저닝하고 관리
    - 클러스터의 노드를 관리하고 Kubernetes API 서버에 대한 엔드포인트를 제공
    - EKS 제어 플레인은 높은 가용성을 제공
    - 클러스터 관리, 보안 업데이트 및 백업을 자동화

**20. 한 보험 회사에서 차량 보험 데이터를 사용하여 위험 분석 기계 학습(ML) 모델을 구축하고 있다. 데이터에는 개인 식별 정보(PII) 가 포함되어 있으며, ML 모델에서 PII가 사용되면 안 된다. 또한 규정에 따라 AWS Key Management Service(AWS KMS) 키로 데이터를 암호화해야 한다. 데이터 엔지니어는 ML 모델에 사용할 보험 데이터를 전달하는 데 적합한 서비스를 선택해야 한다. 다음 중 가장 비용 효율적인 방식으로 이러한 요구 사항을 충족하는 몇 가지 단계는 무엇인가? (2개 선택)**

- AWS KMS(SSE-KMS)를 사용하여 서버 측 암호화로 암호화된 Amazon S3 버킷에 데이터를 전송한다.
- 데이터 수집을 구성하고 PII를 마스킹하는 데 AWS Glue DataBrew를 사용한다.
  - `Amazon S3 서버 암호화`
    - S3 서버가 데이터를 암호화하고 복호화하는 데 사용하는 키를 관리
  - `AWS Glue DataBrew`
    - 코드를 작성할 필요 없이 데이터를 정리하고 정규화할 수 있는 비주얼 데이터 준비 도구
    - 데이터 준비 프로세스 중 PII 데이터를 난독화하는 데이터 마스킹 메커니즘을 제공
    