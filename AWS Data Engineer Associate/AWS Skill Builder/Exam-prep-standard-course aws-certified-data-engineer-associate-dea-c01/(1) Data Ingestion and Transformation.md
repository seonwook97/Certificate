# (1) 데이터 수집 및 변환

## 1-1. 데이터 수집 

### 데이터 생성 단계
- 데이터는 사물인터넷(IoT) 장치, 거래 데이터베이스, 애플리케이션 메시지 큐 등 다양한 소스에서 생성
- 데이터의 특성, 저장 방식, 중복 여부, 스키마 등을 이해해야 함

### 데이터 저장 단계
- 적절한 저장 솔루션을 선택하는 것이 중요
- 다양한 AWS 스토리지 서비스를 이해하고 데이터를 AWS로 수집하는 방법을 알아야 함

### 데이터 수집 단계
- 데이터의 사용 사례, 이동 경로, 수집 빈도 및 볼륨, 데이터 형식 등을 고려해야 함
- 배치와 스트리밍, 푸시와 풀 방식을 이해해야 함
- 데이터 재처리 가능성을 고려하여 파이프라인을 설계하고 테스트해야 함

### 데이터 버전 관리 및 로깅
- 데이터를 내구성 있고 확장 가능한 스토리지 서비스에 저장하여 규정 준수 및 데이터 거버넌스를 보장
- 데이터 수집 파이프라인 내에서 로그, 오류, 메트릭을 캡처하여 진행 상황을 추적하고 실패를 식별

### 인프라스트럭처 코드로 자동화
- AWS CloudFormation 또는 AWS CDK를 사용하여 데이터 수집 파이프라인의 배포 및 구성을 자동화

### AWS 서비스 사용 예시
- Amazon S3, Amazon Kinesis, Amazon EventBridge 등을 사용하여 이벤트 기반으로 데이터를 수집하고 처리
- Amazon Athena를 사용하여 S3에 저장된 데이터를 SQL로 쿼리하고 분석할 수 있음
- Amazon Redshift를 사용하여 데이터 웨어하우스에서 데이터를 분석할 수 있음

### 데이터의 5가지 V
- **다양성**: 데이터의 유형 (웹 서버 로그 파일, 비디오, 사진 등)
- **볼륨**: 데이터의 양과 성장량
- **속도**: 데이터 수집 및 처리 속도
- **정확성**: 데이터의 품질, 완전성, 정확성
- **가치**: 데이터가 비즈니스에 제공하는 가치

### AWS 서비스 사용 사례
- **거래 데이터**: DynamoDB, Amazon RDS, AWS DMS
- **스트리밍 데이터**: Amazon Kinesis, Amazon MSK
- **파일 전송**: AWS Transfer Family
- **온프레미스 데이터**: AWS DataSync
- **대량 데이터**: AWS Snow Family
- **배치 데이터**: Amazon EMR, AWS Glue

--- 

## 1-2. 데이터 변환 및 처리 

### 변환 단계
- **데이터 변환**: 데이터를 원래 형태에서 다른 유용한 형태로 변환하여 보고서, 분석, 머신러닝 등에서 활용할 수 있게 함
- **도구**: 쿼리, 모델링, 변환 도구 사용

### 데이터 모델링
- 데이터 모델링은 데이터 시스템 구축 전에 데이터를 유용하게 조직하는 계획과 설계가 필요
- 다양한 데이터 모델(개념적, 논리적, 물리적)과 정규화, 배치 분석 데이터 모델링, 데이터 볼트 등을 이해해야 함
- **OLTP와 OLAP 데이터 모델링의 차이점 이해**
    - `OLTP`: 거래 처리와 실시간 업데이트 최적화, 최신 상태 데이터 중점
    - `OLAP`: 복잡한 데이터 분석과 보고서 최적화, 최신 상태 및 역사적 데이터 중점

### 데이터 변환 이유
- 데이터 타입 변경, 표준 포맷 적용, 잘못된 데이터 제거 등
- 데이터 스키마 변경, 정규화, 대규모 집계 등을 통한 데이터 통합 및 향상
- 비용 효율성과 비즈니스 가치 고려

### AWS에서의 데이터 변환 및 처리 서비스
- **클라우드 컴퓨팅**: Lambda, Amazon EMR, AWS Glue, Amazon Redshift
- **분산 컴퓨팅**: Amazon EMR, AWS Batch, AWS Step Functions

### 서버리스 처리
- `AWS Glue`: 서버리스 환경에서 데이터 변환 및 처리
- AWS DMS와 AWS Glue Data Catalog를 사용하여 S3 버킷의 데이터를 논리적으로 보기 제공
- Amazon Athena를 사용하여 SQL 쿼리 실행

### Spark를 사용한 데이터 처리
- Amazon EMR 클러스터를 통해 Spark 실행
- Spark 설정 파일 수정, 데이터 처리 애플리케이션 개발, Spark API 사용
- Spark 애플리케이션 제출 및 모니터링

### 데이터 준비 및 변환
- 데이터 준비는 필드 추가, 필터 적용, 필드 이름 변경, 데이터 유형 변경 등 포함
- 데이터가 SQL 데이터베이스에 있는 경우 테이블 조인 가능

### 트러블슈팅 및 성능 최적화
- 로그 확인, 데이터 품질 및 무결성 검증, 소스 데이터와 변환 논리 검증
- 작은 데이터셋을 사용하여 디버깅, 병목 현상 분석, 효율적인 알고리즘 적용
- **인크리멘탈 처리 기술** 및 **리트라이 메커니즘** 구현
 `Incremental Processing`
  - 데이터의 전체 집합이 아닌 변경된 부분만 처리
  - 주기적으로 발생하는 전체 데이터의 반복 처리 대신, 변경된 데이터만 추출하여 효율적으로 처리
  - 데이터 양이 많을 때 처리 시간과 자원을 절약할 수 있음
  - **변경 데이터 캡처(CDC)**
    - 데이터베이스에서 발생한 변경 사항을 추적하여 인크리멘탈 처리를 가능하게 함
    - AWS Database Migration Service(DMS) 등을 사용하여 구현 가능
  - **타임스탬프 기반 필터링**
    - 각 레코드에 타임스탬프를 추가하고, 최근에 변경된 레코드만 필터링하여 처리
  - **버전 관리**
    - 데이터의 버전을 관리하여 최신 버전과의 차이를 기반으로 인크리멘탈 처리 수행
    - S3 버전 관리 기능 등을 사용 가능
    
 `Retry Mechanism`
  - 데이터 처리 중 발생하는 일시적인 오류를 처리하기 위한 전략
  - 일시적인 네트워크 오류, API 제한 초과 등의 상황에서 유용함
  - 오류 발생 시 재시도를 통해 작업을 완료하고, 시스템의 안정성을 높임
  - **백오프 전략**
    - 재시도 간격을 점진적으로 늘려가는 방법
    - 예를 들어, 처음에는 1초, 두 번째는 2초, 세 번째는 4초 등으로 설정
  - **최대 재시도 횟수 설정**
    - 재시도 횟수를 제한하여 무한 루프에 빠지지 않도록 설정
    - 예를 들어, 최대 5번까지 재시도
  - **지수 백오프(Exponential Backoff)**
    - 재시도 간격을 지수 함수 형태로 증가시키는 방법
    - AWS SDK는 기본적으로 지수 백오프를 지원
  - **AWS에서의 리트라이 메커니즘 구현**
    `AWS Step Functions`
      - 상태 기계(State Machine)에서 각 상태의 재시도 정책을 정의 가능
      - 예를 들어, 특정 오류가 발생했을 때 재시도 간격과 횟수를 설정
 
    `AWS Lambda`
      - 재시도 구성을 통해 비동기 실행 시 최대 2번의 재시도 가능
      - AWS Lambda 콘솔이나 AWS CLI를 통해 설정
      
    `Amazon SQS 및 SNS`
      - 메시지를 처리하는 동안 오류가 발생할 경우 재시도 정책을 설정하여 메시지를 다시 시도 
    
### 데이터 API 생성
- **API Gateway**: 데이터 API를 생성하고 관리
- **Lambda**: 데이터 처리 및 API 요청 응답
- **보안 및 인증**: IAM, AWS Certificate Manager, API 키, OAuth 등
- **캐싱 및 성능 최적화**: ElastiCache, API Gateway 캐싱 기능
- **스케일링 및 가용성**: Lambda 함수 자동 스케일링, API Gateway 설정

---

## 1-3. 데이터 파이프라인 오케스트레이션

### 데이터 아키텍처
- 데이터 아키텍처는 조직의 데이터 요구를 지원하기 위해 시스템을 설계하는 것
- 운영 아키텍처는 기능적 요구사항, 기술 아키텍처는 데이터 수집, 저장, 변환, 서빙 방법 포함
- AWS Well-Architected Framework와 여섯 가지 원칙 참고

### 데이터 아키텍처 유형
- 단일 아키텍처 vs 마이크로서비스
- 단일 테넌트 vs 다중 테넌트
- 이벤트 중심 아키텍처, 서버리스, 컨테이너 등
- 데이터 웨어하우스, 데이터 레이크, 데이터 레이크하우스, 데이터 스택, Lambda, Kappa, 데이터 메시 등

### 데이터 파이프라인
- 특정 순서로 실행되어야 하는 데이터 처리 작업의 집합
- 작업은 순차적 또는 병렬로 실행될 수 있으며, 작업 순서는 워크플로우로 정의
- 예: 피드백 데이터를 수집, 검증, 변환, 데이터 웨어하우스에 로드

### 오케스트레이션 도구
- **서버리스 AWS 서비스**
  - Step Functions와 Lambda
- **Step Functions**
  - 상태 기계 언어 사용, 오류 처리, 재시도, 입력/출력, 이벤트 소스, 트리거, 모니터링, 로깅, 테스트, 배포
- **자동화**
  - 데이터 파이프라인 워크플로우의 작업 실행 자동화, 작업 간 종속성 관리

### 파이프라인 트리거링
- 스케줄 기반 파이프라인 vs 이벤트 기반 파이프라인
- 예: 파일 업로드 시 트리거되는 이벤트 기반 파이프라인, 매일 특정 시간에 실행되는 스케줄 기반 파이프라인

### 오류 처리 및 재시도 전략
- 일반적인 오류 원인: 데이터 품질 문제, 코드 오류, 엔드포인트 오류, 종속성 오류 등
- 재시도 전략: 재시도 횟수, 재시도 간격, 백오프율 지정
- 예: Step Functions의 백오프율 사용

### 모니터링 및 알림
- AWS Notification 서비스 사용하여 문제나 오류 발생 시 알림 설정
- Amazon SNS를 사용하여 이메일 알림 전송
- CloudWatch를 사용하여 상태 기계 모니터링

### AWS 서비스
- **서버리스 오케스트레이션 엔진**: AWS Data Pipeline, Step Functions
- **관리형 오픈 소스 프로젝트**: Amazon Managed Workflows for Apache Airflow (MWAA)
- **서비스별 오케스트레이션**: AWS Glue 워크플로우

### 데이터 파이프라인 예시
- **AWS Glue 워크플로우**: AWS Glue 구성 요소만 사용하는 경우 적합
    - 예: AWS Glue 크롤러, Spark 작업, 파이썬 셸 작업 등을 사용한 파이프라인
- **복잡한 워크플로우**: Step Functions 사용
    - 예: 여러 ETL 작업 조정, 사용자 참여와 판매 예측 분석 등

### 간단한 및 복잡한 환경
- **간단한 환경**: AWS Data Pipeline 사용
- **복잡한 환경**: Step Functions, Amazon MWAA 사용

### 모니터링 및 알림
- ETL 작업 완료 후 Amazon SNS를 사용하여 이메일 알림 전송
- CloudWatch를 사용하여 ETL 작업 모니터링

### 성능, 가용성, 확장성, 복원력 및 장애 내성
- 분산 처리 프레임워크 사용 (예: Spark, Amazon EMR, AWS Glue)
- 자동 확장 구성
- 데이터 파티셔닝 기술 구현
- 내결함성 스토리지 서비스 사용 (예: Amazon S3, Amazon EFS)
- 백업 전략 및 계획 구현
- CloudWatch를 사용한 모니터링 및 알림 설정
- 오류 처리 및 재시도 메커니즘 구현
- 데이터 검증 및 품질 검사
- CI/CD 실천법 구현

---

## 1-4. 데이터 프로그래밍 개념 적용 

### 데이터 파이프라인 및 프로그래밍 개념 적용
- 데이터 파이프라인을 유용하게 만드는 방법을 배움
- ETL 과정의 각 단계를 결합하여 데이터 처리 자동화 및 운영

### 데이터 변환 엔진
- AWS는 데이터 변환을 위한 여러 엔진 제공
- 데이터 엔지니어는 원시 데이터셋을 결합하고, 다양한 분석 엔진을 사용하여 새로운 데이터셋 생성

### SQL의 장점
- SQL은 표준 언어로 오랫동안 사용되어 널리 알려져 있음
- 코드 기반 접근 방식, 예를 들어 Spark,는 더 강력하고 다양한 데이터 변환 가능

### Spark 작업 실행 방법
- 배치 또는 근실시간으로 데이터 처리 (Spark Streaming)
- 표준 SQL을 사용하여 데이터 처리 (Spark SQL)
- 머신러닝 기법 적용 (Spark ML)
- AWS Glue를 사용한 서버리스 Spark 실행
- Amazon EMR을 사용한 관리형 Spark 클러스터 배포
- ECS, EKS를 사용한 컨테이너화된 Spark 실행
- Databricks와 같은 AWS 파트너의 관리형 서비스 사용

### Amazon Redshift에서 데이터 변환
- Redshift에서 데이터 변환은 ELT 접근 방식 사용
- SQL을 사용하여 데이터 변환 수행
- Spark SQL을 사용하여 데이터 변환을 오프로드하여 데이터 웨어하우스의 쿼리 성능 최적화

### 데이터 처리 엔진
- Amazon EMR을 사용하여 대규모 데이터셋 처리 및 분석
- 다양한 오픈 소스 소프트웨어 패키지 사용 (Hadoop, Spark, HBase, Presto 등)

### AWS Data Pipeline
- 관리형 ETL 서비스로 다양한 AWS 서비스 및 온프레미스 리소스 간의 데이터 이동 및 변환 정의
- 예: Amazon S3의 클릭스트림 데이터를 Amazon Redshift로 이동

### AWS Glue Studio와 DataBrew
- `AWS Glue Studio`: SQL 기반 변환 작업 설계
- `AWS Glue DataBrew`: 파일 집합에 변환 적용

### Athena로 데이터 쿼리
- Athena는 Amazon S3에 저장된 구조적 및 반구조적 데이터 쿼리
- 컬럼 이름에는 알파벳과 숫자, 밑줄만 사용 가능

### 코드 최적화 전략
- 병렬 처리, 배치 처리, 최적화된 데이터 형식 사용
- 필요한 데이터만 로드 및 처리, 필터링 기술 사용
- 데이터 파티셔닝 및 압축
- 메모리 및 리소스 할당 최적화
- CloudWatch를 사용한 성능 모니터링 및 분석

### 인프라스트럭처 자동화
- CloudFormation 템플릿 및 AWS CDK 사용
- CI/CD를 통한 데이터 파이프라인 구현, 테스트 및 배포
- AWS SAM을 사용한 서버리스 데이터 파이프라인 배포

### Lambda 함수 최적화
- 코드 최적화, 적절한 메모리 및 CPU 설정
- 비동기 호출, 프로비저닝된 동시성 설정
- Amazon VPC에서의 리소스 접근 시 지연 최소화

### 데이터 구조 및 알고리즘
- 데이터 구조와 알고리즘의 이해가 중요
- 데이터 처리 시스템 설계 및 구현 시 성능 최적화, 확장성 향상, 처리 시간 단축
- 다양한 데이터 구조와 알고리즘을 사용하여 효율적인 데이터 처리 시스템 설계

### 데이터 엔지니어의 AWS에서의 프로그래밍 개념 적용
- `데이터 수집`: Lambda를 사용하여 데이터 수집 작업 처리
- `데이터 처리`: Amazon EMR과 AWS Glue를 사용하여 데이터 처리 작업 정의
- `실시간 데이터 처리`: Kinesis를 사용하여 스트리밍 데이터 처리
- `데이터 변환`: AWS Glue를 사용하여 복잡한 변환 수행
- `데이터 저장`: Amazon RDS 또는 Amazon Redshift 사용
- `워크플로우 오케스트레이션`: Step Functions를 사용하여 서버리스 워크플로우 빌드

---

# Reference
- Exam Prep Standard Course: AWS Certified Data Engineer – Associate (DEA-C01)
  - https://explore.skillbuilder.aws/learn/course/18546/exam-prep-standard-course-aws-certified-data-engineer-associate-dea-c01
