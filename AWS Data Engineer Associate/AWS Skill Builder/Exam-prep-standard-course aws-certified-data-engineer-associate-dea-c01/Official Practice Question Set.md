## 공식 문제 세트 20문항

**1. 한 데이터 엔지니어가 AWS Glue 추출, 변환 및 로드(ETL) 파이프라인을 배포하고자 새 계정을 생성했다. 파이프라인 작업을 통해 소스 Amazon S3 버킷에서 원시 데이터를 수집해야 한다. 그런 다음 파이프라인 작업에서, 변환된 데이터를 동일한 계정의 대상 S3 버킷에 작성한다. 데이터 엔지니어는 AWS Glue가 소스 S3 버킷과 대상 S3 버킷에 액세스할 수 있는 권한이 포함된 IAM 정책을 작성했다. ETL 파이프라인을 실행하려면 데이터 엔지니어가 IAM 정책의 권한을 AWS Glue에 부여해야 한다. 다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇인가?** 

- AWS Glue에 새 IAM 서비스 역할을 만든다. 정책을 새 역할에 연결한다. 새 역할을 사용하도록 AWS Glue를 구성한다.

**2. 전자 상거래 회사가 AWS에서 여러 애플리케이션을 실행한다. 이 회사는 중앙 집중식 스트리밍 로그 수집 솔루션을 설계하려고 한다. 솔루션을 사용해 로그 파일을 Apache Parquet 형식으로 변환할 수 있어야 한다. 그런 다음 솔루션으로 Amazon S3에 로그 파일을 저장할 수 있어야 한다. 생성되는 로그 파일의 수는 하루 중에 계속 달라진다. 데이터 엔지니어는 로그 파일을 거의 실시간으로 전달하도록 지원하는 솔루션을 구성해야 한다. 다음 중 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- 로그 파일을 Amazon Kinesis Data Firehose에 전송하도록 애플리케이션을 구성한다. 로그 파일을 Parquet 형식으로 변환하는 AWS Lambda 함수를 호출하도록 Kinesis Data Firehose를 구성한다. Parquet 파일을 출력 S3 버킷에 전송하도록 Kinesis Data Firehose를 구성한다.
  ![image](https://github.com/seonwook97/Certificate/assets/92377162/07453d53-353e-458e-ba7f-377e299116bf)

**3. 한 전자 상거래 회사가 AWS에서 애플리케이션을 실행하고 있다. 이 애플리케이션은 Amazon Redshift의 테이블에서 최신 데이터를 얻고 있다. 1년 이상 된 데이터는 Amazon S3에서 액세스할 수 있다. 최근 SQL로 작성된 새로운 보고서가 있다. 보고서를 비교하려면 올해 매출 테이블의 일부 열과 전년도 매출 데이터가 포함된 테이블에서 동일한 열을 살펴봐야 한다. 보고서 실행 속도가 느리고 성능이 떨어지며 결과를 얻기까지 기다리는 시간이 길다. 데이터 엔지니어가 쿼리를 가속화하려면 백엔드 스토리지를 최적화해야 한다. 다음 중 가장 효과적으로 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- Amazon S3에서 데이터를 수집하도록 보고서 SQL 스테이트먼트를 실행한다. Amazon Redshift 구체화된 뷰에 결과 세트를 저장한다. SQL REFRESH를 실행하도록 보고서를 구성한다. 그런 다음 구체화된 뷰를 쿼리한다.
  - Redshift 물리화된 뷰(Materialized Views)
    - 하나 이상의 기본 테이블에 대한 SQL 쿼리를 기반으로 사전 계산된 결과를 포함합니다.
    - 이는 "일반" 뷰와 달리 실제로 쿼리 결과를 저장한다는 점에서 다릅니다.
    - 특히 대규모 테이블에서 데이터 웨어하우스 환경의 복잡한 쿼리를 가속화하는 방법을 제공합니다.
    - 물리화된 뷰는 다른 테이블이나 뷰처럼 쿼리할 수 있습니다.
    - 쿼리는 기본 테이블에 접근하지 않고 사전 계산된 결과를 사용하므로 더 빠르게 결과를 반환합니다.
    - 이는 예측 가능하고 반복적인 쿼리, 예를 들어 Amazon QuickSight와 같은 대시보드 채우기에 특히 유용합니다.
    ![image](https://github.com/seonwook97/Certificate/assets/92377162/e7b0c7cc-28c3-4b98-bf9e-1ff2a0ae3feb)


**4. 회사에서 데이터를 저장하는 데 Amazon S3 버킷을 활용한다. 이 회사는 새로운 데이터 수명 주기 및 보존 정책을 채택하는 중이다. 정책 정의는 다음과 같다.**
**- 새로 생성된 데이터는 모두 온라인에서 사용할 수 있어야 하고 경우에 따라 SQL로 분석을 수행할 수 있어야 한다.**
**- 3년이 지난 데이터는 안전하게 보관하고 규정 준수 평가를 위해 필요할 때 12시간 이내에 사용할 수 있어야 한다.**
**- 10년 이상 된 데이터는 안전하게 삭제해야 한다.**
**데이터 엔지니어는 데이터 수명 주기 및 보존 정책에 따라 데이터를 비용 효율적으로 저장할 수 있는 솔루션을 구성해야 한다. 다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- S3 Infrequent Access 스토리지 클래스에 새 데이터를 저장한다. Amazon Athena를 사용하여 Amazon S3에 있는 데이터를 해당 위치에서 쿼리한다. 데이터를 3년 후에 S3 Glacier Flexible Retrieval 스토리지 클래스로 마이그레이션하는 수명 주기 규칙을 만든다. 10년 후에 데이터를 삭제하는 수명 주기 규칙을 구성한다.

**5. 회사가 AWS Glue를 사용하여 레코드를 처리하는 데이터 파이프라인을 배포했다. 레코드에는 JSON 형식의 이벤트가 포함되며 경우에 따라 base64로 인코딩된 이미지가 포함되기도 한다. AWS Glue 작업은 10개의 데이터 처리 단위(DPU)로 구성된다. 하지만 AWS Glue 작업은 정기적으로 수백 개의 DPU로 확장되며 실행하는 데 시간이 오래 걸릴 수 있다. 데이터 엔지니어는 데이터 파이프라인을 모니터링하여 적절한 DPU 용량을 결정해야 한다. 다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- AWS Glue 콘솔의 작업 실행 모니터링 섹션을 검사한다. 이전 작업 실행 결과를 검토한다. 적절한 DPU 수를 결정하도록 프로파일링된 지표를 시각화한다. 
  - Glue ETL
    - 기본 Spark 작업의 성능을 향상시키기 위해 추가 "DPU"(데이터 처리 단위)를 프로비저닝할 수 있음
    - 작업 메트릭을 활성화하면 필요한 최대 DPU 용량을 이해하는 데 도움이 됨

**6. 회사가 여러 운영 소스에서 Amazon S3 데이터 레이크로 데이터를 수집한다. 그런 다음 회사는 비즈니스 분석팀이 분석할 수 있도록 데이터를 Amazon Redshift에 수집한다. 비즈니스 분석팀은 최근 3개월간의 고객 데이터에만 액세스할 수 있어야 한다. 또한 회사는 일 년에 한 번 전년도 데이터를 자세히 분석하여 나온 결과를 지난 12개월의 전체 결과와 비교한다. 분석 및 비교한 후에는 더 이상 데이터에 액세스할 수 없다. 그러나 규정을 준수하려면 데이터를 12개월 이후에도 보관해야 한다. 다음 중 가장 비용 효율적인 방식으로 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- 3개월간의 데이터를 Amazon Redshift에 수집한다. 3개월이 지난 데이터는 Amazon Redshift에서 Amazon S3로 언로드 프로세스를 자동화한다. 최대 12개월 이전의 데이터를 포함하여 연간 분석을 수행하는 데 Amazon Redshift Spectrum을 사용한다. Amazon S3에서 수명 주기 정책을 구현하여 12개월이 지난 데이터의 경우, 언로드된 데이터를 S3 Glacier Deep Archive로 이전한다.
  - Redshift Spectrum
    - 데이터를 로드하지 않고 S3에 있는 엑사바이트 규모의 비정형 데이터를 쿼리
    - 무제한 동시성
    - 수평 확장
    - 스토리지와 컴퓨팅 리소스의 분리
    - 다양한 데이터 형식 지원
    - Gzip 및 Snappy 압축 지원
    ![image](https://github.com/seonwook97/Certificate/assets/92377162/9db5f584-e09a-4282-a771-aef4944219ef)

**7. 회사의 데이터가 온프레미스 NFS 파일 공유에 있다. 회사는 AWS로 마이그레이션할 계획이며, 데이터 분석에 데이터가 사용된다. 이 회사는 AWS Lambda 함수를 작성해 데이터를 분석한다. 그리고 Lambda가 액세스하는 파일 시스템에 NFS를 계속 사용하고자 한다. 동시에 실행되는 모든 Lambda 함수 전반에서 데이터가 공유되어야 한다. 다음 중 회사가 이 데이터 마이그레이션에 사용해야 할 솔루션은 무엇인가?**

- 데이터를 Amazon Elastic File System(Amazon EFS)으로 마이그레이션한다. 파일 시스템을 마운트할 수 있도록 Lambda 함수를 구성한다.
  - Amazon EFS – 탄력적 파일 시스템
    - 여러 EC2 인스턴스에 마운트할 수 있는 관리형 NFS(네트워크 파일 시스템)
    - EFS는 여러 가용 영역(AZ)에 걸쳐 EC2 인스턴스와 함께 작동
    - 고가용성, 확장 가능, 비용이 많이 듦(gp2의 3배), 사용량 기준 과금
      ![image](https://github.com/seonwook97/Certificate/assets/92377162/5f86b919-f1bd-47cb-acad-828c9ca3b0a2)

**8. 회사는 사용자가 만든 데이터를 분석용으로 수집하는 데 Amazon S3 데이터 레이크를 사용한다. Amazon S3에서 수집 및 저장되는 일부 데이터에는 개인 식별 정보(PII) 가 포함된다. 이 회사는 데이터 엔지니어가 분석을 수행하기 전에 자동화 솔루션을 설계하여 신규 및 기존 데이터에 포함된 PII에서 마스킹되어야 할 데이터를 식별하기를 바란다. 또한 데이터 엔지니어는 식별된 데이터의 개요를 제공해야 한다. 데이터를 마스킹하는 작업은 AWS 계정에 이미 생성된 애플리케이션에서 처리된다. 데이터 엔지니어는 PII를 찾을 때 실시간으로 이 애플리케이션을 호출할 수 있는 솔루션을 설계해야 한다. 다음 중 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- AWS 계정에서 Amazon Macie를 활성화한다. Macie 결과에 따라 기본 이벤트 버스의 Amazon EventBridge 규칙을 만든다. 마스킹 애플리케이션을 규칙의 대상으로 설정한다.
  - AWS Macie
    - Amazon Macie는 기계 학습과 패턴 매칭을 사용하여 AWS의 민감한 데이터를 발견하고 보호하는 완전 관리형 데이터 보안 및 데이터 프라이버시 서비스입니다.
    - Macie는 개인 식별 정보(PII)와 같은 민감한 데이터를 식별하고 경고하는 데 도움을 줍니다.
      ![image](https://github.com/seonwook97/Certificate/assets/92377162/10297b05-5569-48bf-973c-4febf5821f2a)

**9. AWS에 중앙 집중식 메타데이터 스토리지 솔루션을 배포해야 하는 데이터 엔지니어가 있다. 솔루션은 안정적이고 확장 가능해야 하며, 데이터베이스, 테이블, 열, 행 및 셀 수준에서 세분화된 권한을 제어할 수 있어야 한다. 다음 중 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- 데이터 레이크와 데이터 카탈로그를 생성할 수 있도록 AWS Lake Formation을 사용한다. Lake Formation 데이터 필터를 사용하여 액세스를 제어한다.

  - AWS Lake Formation
    - “며칠 내에 안전한 데이터 레이크를 설정할 수 있도록 도와줍니다”
    - 데이터 로드 및 데이터 흐름 모니터링
    - 파티션 설정
    - 암호화 및 키 관리
    - 변환 작업 정의 및 모니터링
    - 액세스 제어
    - 감사
    - Glue 기반 구축
    ![image](https://github.com/seonwook97/Certificate/assets/92377162/7e7f849a-e967-46fe-af04-5f08418da03c)

**10. 한 금융 회사가 투자 전략 강화를 위해 기계 학습(ML) 모델을 개발했다. 이 모델에서는 주식, 채권 및 원자재 시장과 관련된 다양한 데이터 소스가 사용된다. 모델은 프로덕션 승인을 받았다. 데이터 엔지니어는 ML 의사결정을 실행하는 데 사용되는 데이터가 정확하고 완전하며 신뢰할 수 있는지를 확인해야 한다. 데이터 엔지니어는 모델 프로덕션 배포를 대비한 데이터 준비를 자동화해야 한다. 다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- 모델에 사용할 데이터를 준비할 수 있도록 Amazon SageMaker ML 계보 추적 단계와 함께 Amazon SageMaker 워크플로를 사용한다.
  - SageMaker ML 계보 추적
    - ML 워크플로우(MLOps)를 생성하고 저장합니다
    - 모델의 실행 기록을 유지합니다
    - 감사 및 준수를 위한 추적
    - 자동 또는 수동으로 생성된 추적 엔터티
    - 계정 간 계보를 위한 AWS Resource Access Manager와 통합
    - SageMaker에서 생성된 계보 그래프 샘플
    ![image](https://github.com/seonwook97/Certificate/assets/92377162/cec5f721-072a-4f71-a10f-d4b5a993b3b0)

**11. 회사가 AWS에서 Amazon Redshift 데이터 웨어하우스를 운영하고 있다. 이 회사는 최근 여러 AWS 서비스에서 지원하는 서비스형 소프트웨어(SaaS) 영업 애플리케이션의 사용을 시작했다. 그리고 보고 목적으로 SaaS 애플리케이션의 일부 데이터를 Amazon Redshift로 전송하고자 한다. 데이터 엔지니어는 SaaS 애플리케이션에서 Amazon Redshift로 데이터를 지속적으로 전송할 수 있는 솔루션을 구성해야 한다. 다음 중 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- 선택된 소스 데이터를 Amazon Redshift로 수집할 수 있도록 Amazon AppFlow 플로를 만든다. 이벤트에서 실행되도록 플로를 구성한다.
  - Amazon AppFlow
    - SaaS(Software-as-a-Service) 애플리케이션과 AWS 간에 데이터를 안전하게 전송할 수 있는 완전 관리형 통합 서비스입니다
    - 소스: Salesforce, SAP, Zendesk, Slack, ServiceNow
    - 대상: Amazon S3, Amazon Redshift와 같은 AWS 서비스 또는 SnowFlake, Salesforce와 같은 비AWS 서비스
    - 빈도: 일정에 따라, 이벤트에 응답하여, 또는 필요할 때마다
    - 필터링 및 검증과 같은 데이터 변환 기능
    - 공용 인터넷을 통한 암호화 또는 AWS PrivateLink를 통한 개인 네트워크를 통한 전송
    - 통합 작업을 직접 작성하는 데 시간을 낭비하지 않고 API를 즉시 활용할 수 있습니다
    ![image](https://github.com/seonwook97/Certificate/assets/92377162/ea5acca3-884d-4286-893c-5bcb2a554790)

**12. 한 금융 회사가 Amazon S3 버킷에 지불이 끝난 인보이스를 저장하고 있다. 인보이스가 업로드되면 AWS Lambda 함수에서 Amazon Textract를 사용하여 PDF 데이터를 처리하고 데이터를 Amazon DynamoDB에 보관한다. 현재 Lambda 실행 역할에는 다음의 S3 권한이 있다.**
```JSON
{

    "Version": "2012-10-17",

    "Statement": [

        {

            "Sid": "ExampleStmt",

            "Action": ["s3:*”],

            "Effect": "Allow",

            "Resource": ["*"]

        }

    ]

}
```
**이 회사는 보안 모범 사례에 따라 Amazon S3에 역할 권한을 지정하여 수정하고자 한다. 다음 중 이러한 요구 사항을 충족하는 솔루션은 무엇인가?**

- Action을 "s3:GetObject"로 수정한다. Resource가 버킷 ARN만 되도록 수정한다.